<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-123499302-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'UA-123499302-2');
  </script>
  <title>Gradio | Getting Started</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <link href="static/home/style/style.css" rel="stylesheet">
  <link href="static/css/prism.css" rel="stylesheet">
</head>

<body>
  <header class="container">
    <img id="logo" src="static/home/img/logo_inline.png" />
    <nav>
      <a href="/">Home</a>
      <a class="selected" href="/getting_started">Getting Started</a>
      <a href="/hub">Hub</a>
      <a href="/#contact-box">Contact</a>
    </nav>
  </header>
  <main>
    <div class="container">
      <h2>Getting Started</h2>
      <p>Gradio can wrap almost any Python function with an easy to use interface. Let's take a look at a few examples.</p>
    <h3> 0. Hello World

      <a
            href="https://colab.research.google.com/drive/18ODkJvyxHutTN0P5APWyGFO_xwNcgHDZ?usp=sharing"><img
            src="https://colab.research.google.com/assets/colab-badge.svg"
      /></a></h3>

      <p>Let's start with a basic function (no machine learning yet!) that greets an input name. We'll
          wrap the function with a <code>Text</code> to <code>Text</code> interface.</p>
<pre><code class="lang-python">import gradio as gr

def greet(name):
  return "Hello " + name + "!"

gr.Interface(fn=greet, inputs="text", outputs="text").launch()</code></pre>
      <p>The core <code>Interface</code> class is initialized with three parameters:
          <ul>
            <li><code>fn</code>: the function to wrap</li>
            <li><code>inputs</code>: the name of the input interface</li>
            <li><code>outputs</code>: the name of the output interface</li>
          </ul>
        Calling the <code>launch()</code> function of the <code>Interface</code> object produces the interface shown in
        image below. </p>
        <div class="screenshot">
        <img src="static/home/img/hello-world.png">
        </div>

    <h3> 1. MobileNet

      <a
            href="https://colab.research.google.com/drive/1c6gQiW88wKBwWq96nqEwuQ1Kyt5LejiU?usp=sharing"><img
            src="https://colab.research.google.com/assets/colab-badge.svg"
      /></a></h3>

      <p>Now, let's do a machine learning example. We're going to wrap an interface around the image classifier MobileNet! Since this is an image classification model, we will use the <code>Image</code> input interface. We'll output a dictionary of labels and their corresponding confidence scores with the <code>Label</code> output interface.</p>

<pre><code class="lang-python">import gradio as gr
import tensorflow as tf
import numpy as np
import requests

mobile_net = tf.keras.applications.MobileNetV2()  # Load the model.

# Download human-readable labels for ImageNet.
response = requests.get("https://git.io/Jep2M")
idx_to_labels = {index : name.split('\'')[1].split(',')[0] for index, name in enumerate(
    response.text.split("\n"))}

def classify_image(inp):
  inp = np.expand_dims(inp, 0)
  prediction = mobile_net.predict(inp).flatten()
  return {idx_to_labels[i]: float(prediction[i]) for i in range(1000)}

image = gr.inputs.Image(shape=(224, 224, 3))
label = gr.outputs.Label(num_top_classes=3)

gr.Interface(fn=classify_image, inputs=image, outputs=label).launch()</code></pre>
      <p>This code will produce the interface below. The interface gives you a way to test
      Inception Net by dragging and dropping images, and also allows you to use naturally modify the input image using image editing tools that
          appear when you click EDIT. Notice here we provided actual <code>gradio.inputs</code> and <code>gradio.outputs</code> objects to the Interface
          function instead of using string shortcuts. The lets us use built-in preprocessing (e.g. image resizing)
          and postprocessing (e.g. choosing the number of labels to display) provided by these
            interfaces.
          Try it out in your device or run it in a <a href="https://colab.research.google.com/drive/1c6gQiW88wKBwWq96nqEwuQ1Kyt5LejiU?usp=sharing" target="_blank">colab notebook</a>!</p>

        <div class="screenshot">
        <img src="static/home/img/inception-net.png">
        </div>

    <h3> 2. Real-Time MNIST

      <a
            href="https://colab.research.google.com/drive/1LXJqwdkZNkt1J_yfLWQ3FLxbG2cAF8p4?usp=sharing"><img
            src="https://colab.research.google.com/assets/colab-badge.svg"
      /></a></h3>

        <p>Let's wrap a fun <code>Sketchpad</code>-to-<code>Label</code> UI around MNIST. For this example, we'll take advantage of the <em>live</em>
            feature in the library. Set <code>live=True</code> inside <code>Interface()</code> to have it run continuous predictions.
            We've abstracted the model training from the code below, but you can see the full code on the colab link.</p>
<pre><code class="lang-python">import tensorflow as tf
import gradio as gr

model = tf.keras.models.load_model('models/mnist.h5')

def recognize_digit(inp):
    prediction = model.predict(inp.reshape(1, 28, 28, 1)).tolist()[0]
    return {str(i): prediction[i] for i in range(10)}

sketchpad = gr.inputs.Sketchpad()
label = gr.outputs.Label(num_top_classes=3)

gr.Interface(fn=recognize_digit, inputs=sketchpad,
  outputs=label, live=True).launch()</code></pre>
      <p>This code will produce the interface below.</p>
      <div class="screenshot">
        <img src="static/home/img/mnist-live.png">
      </div>

        <h3> 3. GPT-2

      <a
            href="https://colab.research.google.com/drive/1fmt1L6qBzEQo6Nc5t4SPxRCrh7FvxIVc?usp=sharing"><img
            src="https://colab.research.google.com/assets/colab-badge.svg"
      /></a></h3>

      <p>Let's wrap a text to text interface around GPT-2, a text generation model that works on provided starter text.</p>
<pre><code class="lang-python">import gradio as gr
import tensorflow as tf
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = TFGPT2LMHeadModel.from_pretrained("gpt2", pad_token_id=tokenizer.eos_token_id)

def generate_text(inp):
    input_ids = tokenizer.encode(inp, return_tensors='tf')
    beam_output = model.generate(input_ids, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)
    output = tokenizer.decode(beam_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)
    return ".".join(output.split(".")[:-1]) + "."

output_text = gr.outputs.Textbox(lines=15)
gr.Interface(generate_text,"textbox", output_text).launch()</code></pre>
    <p>This code will produce the interface below. That's all that's needed!</p>
    <div class="screenshot">
      <img src="static/home/img/gpt-2.png">
    </div>

    <h3> 4. Multiple Inputs: BERT-QA
      <a
            href="https://colab.research.google.com/drive/1hYIwl9elB6kv2n7Yl9CpnOYf14hzs9el?usp=sharing"><img
            src="https://colab.research.google.com/assets/colab-badge.svg"
      /></a></h3>

      <p>What if our model takes more than one input? Let's wrap a 2-input to 1-output interface around BERT-QA. </p>
<pre><code class="lang-python">import gradio as gr
from bert import QA

model = QA('bert-large-uncased-whole-word-masking-finetuned-squad')

def qa_func(context, question):
    return model.predict(context, question)["answer"]

gr.Interface(qa_func,
    [
        gr.inputs.Textbox(lines=7),
        gr.inputs.Textbox(lines=1),
    ],
    gr.outputs.Textbox(lines=7, label="Answer")).launch()</code></pre>
    <p>As shown in the code, Gradio can wrap functions with multiple inputs or outputs, simply by taking the list of components needed.
        The number of input components should match the number of parameters taken by <code>fn</code>. The number of
        output components should match the number of values returned by <code>fn</code>. This code will produce the interface below.</p>
        <div class="screenshot">
        <img src="static/home/img/bert-qa.png">
        </div>
        <h3> 5. Numerical Interface: Titanic Model

      <a
            href="https://colab.research.google.com/drive/1yMp-YBfJgyaKG3vMDmDtuCmm4YGY1R6X?usp=sharing"><img
            src="https://colab.research.google.com/assets/colab-badge.svg"
      /></a></h3>

      <p>Many models have numeric or categorical inputs, which we support with a variety of interfaces.
          Let's wrap multiple input to label interface around a Titanic survival model. We've hidden some of
          model-related code below, but you can see the full code on colab.
      </p>
<pre><code class="lang-python">import gradio as gr
import pandas as pd
import numpy as np
import sklearn

def predict_survival(sex, age, fare):
    df = pd.DataFrame.from_dict({'Sex': [sex], 'Age': [age], 'Fare': [fare]})
    df = encode_sex(df)
    df = encode_fares(df)
    df = encode_ages(df)
    pred = clf.predict_proba(df)[0]
    return {'Perishes': pred[0], 'Survives': pred[1]}

sex = gr.inputs.Radio(['female', 'male'], label="Sex")
age = gr.inputs.Slider(minimum=0, maximum=120, default=22, label="Age")
fare = gr.inputs.Slider(minimum=0, maximum=1000, default=100, label="Fare (british pounds)")

gr.Interface(predict_survival, [sex, age, fare], "label", live=True).launch()</code></pre>
    <p>This code will produce the interface below. </p>
        <div class="screenshot">
        <img src="static/home/img/titanic.png">
        </div>

      <h3>6. Multiple Models: Comparing Inception to MobileNet

      <a
            href="https://colab.research.google.com/drive/1LsMyL1ksbeEuWrkbCgWWKCVO9Nenv9nA?usp=sharing"><img
            src="https://colab.research.google.com/assets/colab-badge.svg"
      /></a></h3>
      <p>What if we want to compare two models? The code below generates a
        UI that takes one image input and returns predictions from both
        Inception and MobileNet. We'll also add a title and a description. In the colab notebook, we also include
          sample images.</p>
      <pre><code class="lang-python">import gradio as gr
from imagenetlabels import idx_to_labels
import tensorflow as tf
import numpy as np


mobile_net = tf.keras.applications.MobileNetV2()
inception_net = tf.keras.applications.InceptionV3()


def classify_image_with_mobile_net(im):
    im = im.convert('RGB')
    im = im.resize((224, 224))
    arr = np.array(im).reshape((-1, 224, 224, 3))
    arr = tf.keras.applications.mobilenet.preprocess_input(arr)
    prediction = mobile_net.predict(arr).flatten()
    return {idx_to_labels[i].split(',')[0]: float(prediction[i]) for i in range(1000)}


def classify_image_with_inception_net(im):
    im = im.convert('RGB')
    im = im.resize((299, 299))
    arr = np.array(im).reshape((-1, 299, 299, 3))
    arr = tf.keras.applications.inception_v3.preprocess_input(arr)
    prediction = inception_net.predict(arr).flatten()
    return {idx_to_labels[i].split(',')[0]: float(prediction[i]) for i in range(1000)}


imagein = gr.inputs.Image(cast_to="pillow")
label = gr.outputs.Label(num_top_classes=3)

gr.Interface(
    [classify_image_with_mobile_net, classify_image_with_inception_net],
    imagein,
    label,
    title="MobileNet vs. ImageNet",
    description="Let's compare 2 state-of-the-art machine learning models that classify images into one of 1,000 categories: MobileNet (top), a lightweight model that has an accuracy of 0.704, vs. InceptionNet (bottom), a much heavier model that has an accuracy of 0.779."
).launch();</code></pre>
      <p>This code will produce the interface below. Looks like MobileNet is
          considerably faster! </p>
      <div class="big_screenshot" >
        <img src="static/home/img/inception-mobile.png">
      </div>


        <h2 id="interfaces">List of Gradio Interfaces: </h2>
        Here's a list of the interfaces we currently support, along with their preprocessing / postprocessing
        parameters:
        <h4> Input Interfaces</h4>
        <ul>
            <li><code>Sketchpad(shape=(28, 28), invert_colors=True, flatten=False, scale=1/255, shift=0,
                 dtype='float64')</code></li>
            <li><code>Webcam(image_width=224, image_height=224, num_channels=3, label=None)</code></li>
            <li><code>Textbox(lines=1, placeholder=None, label=None, numeric=False)</code></li>
            <li><code>Radio(choices, label=None)</code></li>
            <li><code>Dropdown(choices, label=None)</code></li>
            <li><code>CheckboxGroup(choices, label=None)</code></li>
            <li><code>Slider(minimum=0, maximum=100, default=None, label=None)</code></li>
            <li><code>Image(cast_to=None, shape=(224, 224, 3), image_mode='RGB',
                 scale=1/127.5, shift=-1, label=None)</code></li>
            <li><code>Microphone()</code></li>
        </ul>
        <h4> Output Interfaces</h4>
        <ul>
            <li><code>Label(num_top_classes=None, label=None)</code></li>
            <li><code>Image(label=None)</code></li>
            <li><code>Microphone(lines=1, placeholder=None, label=None)</code></li>
            <li><code>Image(label=None, plot=False)</code></li>
        </ul>

        <p>
        <a href="/#contact-box">Let us know</a> what interfaces we should add next, or feel free to
        contribute to the  <a href="https://github.com/gradio-app/gradio-UI" target="_blank">open-source library</a>.
        </p><p>&nbsp;</p>

        <h2 id="sharing">Sharing your Gradio Interfaces: </h2>
        <p>
            Interfaces can be shared by setting <code>share=True</code> in the
            <code>launch()</code> method. Like this: </p>

        <pre><code class="lang-python">gr.Interface(classify_image, image, label).launch(share=True)</code></pre>

            <p>
            This generates a public, shareable link that you can send to anybody! When you send this link, the user
            on the other side can try out the model in their browser. Because the processing happens on your device
            (as long as your device stays on!), you don't have to worry about any dependencies.

            If you're working out of colab notebook, a share link is always
            created. Links expire after 8 hours. Need longer links? <a href="/#contact-box">Contact us for Gradio Teams</a>.
        </p>

        <div class="screenshot" >
        <img src="static/home/img/sharing.png">
      </div>

    </div>
  </main>


    <script src="static/js/prism.js"></script>

</body>

</html>