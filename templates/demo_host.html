<html>

<head>
  <title>Gradio | Saliency Cropping</title>
  {% include 'header.html' %}
  <link href="/static/home/style/hub.css" rel="stylesheet">
</head>

<body id="hub_host">
  <div id="header_holder" class="container">
    <header class="container">
      <a href="/" id="back">
        <img src="/static/home/img/back.png">
        <span>Go to Gradio</span>
      </a>
      <div id="gradio-tag">
        <a href="/" class="primary">
          <img id="logo" src="/static/home/img/logo_host.png">
        </a>
        <div id="star-link">
          <a class="github-button" href="https://github.com/gradio-app/gradio-UI" data-icon="octicon-star" data-size="small" data-show-count="true" aria-label="Star gradio-app/gradio-UI on GitHub">Star</a>
        </div>
      </div>
    </header>
  </div>
  <h1 class="title">Investigating Saliency Based Image Cropping</h1>

  <p class="subtitle">By <a href="https://www.vinayprabhu.com/">Vinay
    Prabhu</a>, <a
            href="https://abebabirhane.wordpress.com/">Abeba Birhane</a>, <a
            href="https://twitter.com/IDoTheThinking">Darrell Owens</a>, and
    the <a href="https://gradio.app/#contact-box">Gradio Team</a>.</p>
  <p class="note">How to use this demo: 1. Upload your image (or click to
    load an example). 2. You can click 'Edit' to use
    tools like crop, flip, and add text on your image. 3. Click 'Submit' to
    view
    the
    cropped image. 4. Try submitting with "Show Saliency Map" on if you want
    to see
    why the model chose to crop there. 5. Clicking 'Screenshot' will
    download a
    screenshot of
    your input and output in your browser.</p>

  <div id="interface"></div>

  <p class="text">
    Last week, the saliency-based image cropping algorithm deployed by
    twitter came into <a
          href="https://www.theguardian.com/technology/2020/sep/21/twitter-apologises-for-racist-image-cropping-algorithm">scrutiny</a>. Inspired by some of the <a href="https://medium.com/@VinayPrabhu/on-the-twitter-cropping-controversy-critique-clarifications-and-comments-7ac66154f687">conversations</a>
                         that
    unraveled on Twitter and the widely shared reported incidents of racial
    discrimination, we sought to investigate, experiment, and elucidate the
    workings of cropping algorithms.</p>

  <span style="font-weight:bold"><p class="text">
    Democratizing the audit
  </p></span>

  <p class="text">
    In order to democratize the scrutiny of this technology, we have created an educational saliency based cropping app where you can upload images and see what a state-of-the-art machine learning model similar to the one deployed by twitter thinks are important parts of the image and see how that results in what parts of the image are cropped out. (Please note that, the exact model and the cropping policy used by twitter are both, to the best of our knowledge, proprietary and beyond easy access. Therefore, our reconstruction is limited to what is available in peer-reviewed open sourced academic literature). We have also added an interactive TOAST UI image editor that one can use to further explore the brittleness of this technology.
  </p>

  <span style="font-weight:bold"><p class="text">
  On saliency based cropping
  </p></span>


  <p class="text">
  Saliency based cropping is not unique to Twitter. This very same
    technology is also used by other tech firms including <a
          href="https://twitter.com/AnimaAnandkumar/status/1308096236159893505?s=20">Google</a>,
                                                             <a
                                                                     href="https://patents.google.com/patent/US9626584B2/en">Adobe</a>, and
    <a
            href="https://developer.apple.com/documentation/vision/cropping_images_using_saliency">Apple</a>. This technique, which twitter <a href="https://blog.twitter.com/engineering/en_us/topics/infrastructure/2018/Smart-Auto-Cropping-of-Images.html">admittedly uses</a> on it’s
       platform, typically entails two phases: The
          saliency mask
                                                      estimation phase and the cropping phase.
  </p>


    <p class="text">In the first phase, a saliency mask is estimated using a
      machine
    learning model that ingests an input image and speculates which parts of
    the image are interesting and/or important (retain-worthy) and which
    parts of the image are discardable (or crop-worthy). These machine
    learning models are typically trained on datasets such as <a
              href="http://salicon.net/">SALICON</a>,
      <a href="http://saliency.mit.edu/datasets.html">MIT-1003</a> and CAT2000
      with attention-annotated “ground truth”
       saliency
    maps collected by either using volunteers or crowd-sourcing exercises
      .</p>
<p class="text">In the second phase, the saliency map output in the first
    phase
    is
          then used to come up with a cropping policy that results in a
    cropped image with the so-perceived non-salient parts of the image being
    removed and the so-perceived salient parts of the image being retained
    .</p>

    <p class="text">
As it turns out, this cropping process is a double edged sword. As it is evident in these example images, even the cropped image seems fair, the cropping has in fact, masked the differential saliency that the machine learning model associates with the different constituent faces in the image and some of these nuanced facets of biased ugliness are obfuscated in the finally rendered image.
  </p>

    <span style="font-weight:bold"><p class="text">
On the saliency model we used on the gradio app
    </p></span>

  <p class="text">
  Given that both twitter’s saliency-estimation model and the cropping
    policy are not in the public domain, we used a similar model from
    peer-reviewed machine learning literature that emulates twitter’s
    cropping algorithm. We looked for a SoTA model that was open-sourced. We
    used the <a href="https://github.com/alexanderkroner/saliency">MSI-Net</a>
                model which ranked high on the <a
          href="https://saliency.tuebingen.ai/results.html">MIT/Tuebingen
    Saliency Benchmark</a>. The associated paper is <a
          href="https://www.sciencedirect.com/science/article/pii/S0893608020301660">Contextual
          Encoder–Decoder
    Network for Visual Saliency Prediction</a> by Kroner et al. Since this
    model only maps an input image to saliency map, and doesn’t perform any
    cropping, we authored a cropping function which is a sliding window with
    a fixed aspect ratio (16,9) that maximizes sum of saliency. Our code is
    open-sourced, and you can find everything required to build this
    interface <a href="https://github.com/gradio-app/saliency">here</a>.

  </p>
   <span style="font-weight:bold"><p class="text">
Participation
   </p></span>
   <p class="text">
The gradio saliency based image cropping app is open for anyone to interact and experiment with. Upload an image and simply click the submit button, which will show you a heatmap of features that the algorithm picks up as “important”. We do not save or store your images.
If you come across an unusual, discriminatory, or biased saliency
     distribution that you’d like for us to pay heed to, please let us know
     by dropping it <a href="https://t.co/mY6I2sXRGZ?amp=1">here</a>. (However,
     please make sure that the images
                       that
     you are uploading are consensually sourced and adhere to <a
           href="https://creativecommons.org/about/cclicenses/">CC-BY
     regulations</a>.)
  </p>

<script src="/static/home/js/github-buttons.js"></script>
{% include 'footer.html' %}
<script>
  let model_url = "{{ model_url }}"
  let config = JSON.parse({{ model_config|tojson }});
  gradio_url(config, `${model_url}api/`, "#interface", model_url + "file/");
</script>
</body>

</html>
